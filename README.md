# TweetCorpus

Python 3 class for working with (large sets of) tweets. Does tokenizing, token classification, language guessing for stopword classification (using langdetect), stemming (using Porter2), and sentiment analysis (using vaderSentiment).

I made this class to quickly prepare a large body of tweets for e.g., network analysis or sentiment analysis. Thus, special care is taken to extract hashtags and mentions correctly. However, other than this, the program doesn't make any assumptions about how the output is used and it should be very flexible.

The core of the class is the classify() method, which records each token's position, classifies it (e.g., as a hashtag, mention, stopword, contentword, punctuation, etc.) and stems it (only for some tokens). It allows the user to quickly extract "All mentions that occur at the beginning of a tweet", "Hashtag co-occurrences", "All tokens which have 'function' as their stem" and "The frequency distribution of emoticons" and the like.

All functions include a parallelized version (for maximum speed) and a version based on generator functions (for minimum memory usage). With parallelization disabled, it generates a CSV file of all stemmed and classified tokens for 1,3M tweets in ~24 minutes on a dual core i5 dual laptop with 8GB memory; with parallelization enabled, on a smaller, 150K Tweets dataset, it generates a vocabulary in 70 seconds, generates a CSV of the token classification in ~1.5 minutes, and does sentiment analysis in ~2.5 minutes.

# Typical usage

The default is to use the parallelized version of all functions for maximum speed. This should be fine for data sets up to 250K tweets (with 8GB memory).

```python
from TweetCorpus import tweetcorpus

with open(filename) as f:
    tweets = f.readlines()

twc = tweetcorpus.TweetCorpus(tweets)

# get a vocabulary, which can then be used to do lexicon-based flagging of truncated words, hashtags, etc
vocab = twc.build_vocabulary()

# instantiate new class with vocabulary (the vocabulary helps the class detect if words / mentions / hashtags are truncated or not)
twc = tweetcorpus.TweetCorpus(tweets, vocabulary = vocab)

# output classified tokens to csv, for further processing in, e.g., R:
twc.tokens_csv('tokens_export.csv')

# or, to get only the tokens for further processing in python:
tokens = twc.tokenize()

# to get the token classification for further processing in python:
ctokens = twc.classify()

# do sentiment analysis and export result
twc.sentiments_csv('sentiments_export.csv')

# or, to do sentiment analysis for further processing in python:
sentiments = twc.analyze_sentiments()

```

# Usage with a large dataset in csv format

For larger datasets (unlimited number of tweets), use the non-parallellized functions based on generators.
For example:

```python
import csv
from TweetCorpus import tweetcorpus
from TweetCorpus import tweetcorpus

def stream_tweets(filename):
    with open(filename) as f:
        reader = csv.DictReader(f)
        for row in reader:
            yield row['text']

twc = tweetcorpus.TweetCorpus(stream_tweets('/path/to/file'), parallel=False) # turn off parallellisation to work through the tweets serially, which reduces memory consumption enormously

# write tokens to csv
twc.tokens_csv('tokens_export.csv')

```

# Tips

* Internally, the class uses persistent tweet IDs. These are automatically generated by default, but to use your own tweet IDs, just pass the corpus as a list of tuples (tweet_id, tweet_text).
* Switch off parallellisation to work with huge files (>250K tweets) by passing parallel = False when instantiating the class

# Credits

This class is built on the twokenize tokenizer (I use my own ark-twokenize-py fork, also on github, based on the one by Sentimentron for a working Python 3 version, https://github.com/mheerdink/ark-twokenize-py), the vaderSentiment library for sentiment analysis, and langdetect and pycountry for automated language detection.

Some regular expressions and ideas were borrowed from tweet-preprocessor, https://github.com/s/preprocessor (and my own fork, https://github.com/mheerdink/preprocessor)
